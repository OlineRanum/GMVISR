data:
    variables:
        # Set model name
        MODEL_NAME: &model_name gmvisr_slgcn
        PROJECT_NAME: &project_name gmvisr_slgcn


        # Set Data paths
        OUTPUT_PATH: &output_path new_models/
        DATA_PATH: &data_path data/metadata_kfold/1_2_3/T1/metadata_fold_2.json
        POSE_PATH: &pose_path path/to/poses/Poses/
        TARGET_PATH: &target_path  src.models.slgcn.openhands.datasets.isolated.WLASLDataset

        # Set modality
        MOD: &modality "pose" 


        # Select phonological parameters
        PARAMETERS: &params  [
                #"Handshape"
                # "Selected Fingers"
                # "Flexion",
                # "Spread",
                # "Spread Change",
                # "Thumb Position",
                # "Thumb Contact",
                # "Sign Type",
                # "Path Movement",
                # "Repeated Movement",
                # "Major Location",
                # "Minor Location"
                # "Second Minor Location",
                # "Contact",
                # "Nondominant Handshape", 
                # "Wrist Twist",
                # "Handshape Morpheme 2"
            ]


    modality: *modality
    
    train_pipeline:
        dataset:
            _target_: *target_path
            split_file: *data_path
            root_dir: *pose_path
            splits: "train"
            modality: *modality

        transforms:
            - PoseSelect:
                preset: mediapipe_holistic_minimal_27
            - CenterAndScaleNormalize:
                reference_points_preset: shoulder_mediapipe_holistic_minimal_27
                scale_factor: 1

        dataloader:
            _target_: torch.utils.data.DataLoader
            batch_size: 32
            shuffle: true
            num_workers: 6
            pin_memory: true
            drop_last: true
        
        parameters: *params
        
       
    
    valid_pipeline:
        dataset:
            _target_: *target_path
            split_file: *data_path
            root_dir: *pose_path
            splits: "valid"
            modality: *modality

        transforms:
            - PoseSelect:
                preset: mediapipe_holistic_minimal_27
            - CenterAndScaleNormalize:
                reference_points_preset: shoulder_mediapipe_holistic_minimal_27
                scale_factor: 1

        dataloader:
            _target_: torch.utils.data.DataLoader
            batch_size: 32
            shuffle: true
            num_workers: 3
            pin_memory: true
            drop_last: false
        
        parameters: *params
        
       

model:
    encoder:
        type: decoupled-gcn
        params:
            # pretrained adapters to fuse (only makes sense when learn_adapter is true)
            adapters: []
            # learns an adapter for the selected CLF heads. if there isn't a `pretrained` option on line 1, it'll learn all params (model+adpter)
            learn_adapter: false

            graph_args:
                num_nodes: 27
                inward_edges:
                    [
                        [2, 0],
                        [1, 0],
                        [0, 3],
                        [0, 4],
                        [3, 5],
                        [4, 6],
                        [5, 7],
                        [6, 17],
                        [7, 8],
                        [7, 9],
                        [9, 10],
                        [7, 11],
                        [11, 12],
                        [7, 13],
                        [13, 14],
                        [7, 15],
                        [15, 16],
                        [17, 18],
                        [17, 19],
                        [19, 20],
                        [17, 21],
                        [21, 22],
                        [17, 23],
                        [23, 24],
                        [17, 25],
                        [25, 26],
                    ]
    decoder:
        type: param_fc
        params:
            dropout_ratio: 0
        parameters: *params

optim:
    loss: 'CrossEntropyLoss'
    optimizer:
        name: Adam
        params:
            lr: 1e-3

    scheduler:
        name: CosineAnnealingLR
        params:
            last_epoch: -1
            T_max: 10

trainer:
    max_epochs: 250
    # resume_from_checkpoint: /path/to/model.ckpt

exp_manager:
    create_tensorboard_logger: true
    create_wandb_logger: true
    wandb_logger_kwargs:
        name: *model_name
        project: *project_name

    create_checkpoint_callback: true
    checkpoint_callback_params:
        monitor: "val_acc"
        mode: "max"
        save_top_k: 1
        dirpath: *output_path

    early_stopping_callback: true
    early_stopping_params:
        monitor: "val_acc"
        patience: 100
        verbose: true
        mode: "max"
